{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c3012f-408c-4eee-aa6b-507d075d7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pymlpipe.tabular import PyMLPipe\n",
    "from pymlpipe.pymlpipeUI import start_ui\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "\n",
    "from helper import objective\n",
    "from helper import get_model \n",
    "from helper import evaluate_for_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ecfcc8-3912-4db4-9c17-28961a1d1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_DATA_RAW = 'data raw'\n",
    "REPOSITORY_DATA_PREPROCESSED = 'data preprocessed'\n",
    "REPOSITORY_STUDIES = 'studies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e23686-439f-4304-9498-c210e940b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = PyMLPipe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059dbed-6531-4b87-a1e1-cb9f20ac0623",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53119a8b-2426-48b5-afe5-98248f142abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PREVIOUS_HOUR_VALUES = 720\n",
    "N_PREVIOUS_DAY_VALUES = 90\n",
    "N_PREVIOUS_WEEK_VALUES = 50\n",
    "N_PREVIOUS_MONTH_VALUES = 36\n",
    "N_FEATURES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac16a0-1c3f-476c-b56e-af649bfccb2b",
   "metadata": {},
   "source": [
    "Read raw dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d0b3aa-ac2c-40a9-983a-695d7c471f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ask = pd.read_csv('./data raw/FOREX_EUR_CHF_ASK.csv')\n",
    "df_bid = pd.read_csv('./data raw/FOREX_EUR_CHF_BID.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7577717-d6b1-4035-b525-b8933dcb9442",
   "metadata": {},
   "source": [
    "Rename columns with bid and ask prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028fe02b-f5a6-4b8e-b240-7489ae501dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ask.rename(\n",
    "    columns={\n",
    "        'Open': 'Ask_Open',\n",
    "        'High': 'Ask_High',\n",
    "        'Low': 'Ask_Low',\n",
    "        'Close': 'Ask_Close',\n",
    "        'Volume': 'Ask_Volume'\n",
    "    },\n",
    "    inplace=True)\n",
    "\n",
    "df_bid.rename(\n",
    "    columns={\n",
    "        'Open': 'Bid_Open',\n",
    "        'High': 'Bid_High',\n",
    "        'Low': 'Bid_Low',\n",
    "        'Close': 'Bid_Close',\n",
    "        'Volume': 'Bid_Volume'\n",
    "    },\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f4ffd-bf09-4b6a-83fe-a539d095ebff",
   "metadata": {},
   "source": [
    "Merge dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36630fe7-29ee-4fbd-9281-939afba7d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forex = df_ask.merge(df_bid, left_on='Gmt time', right_on='Gmt time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e9f1c-a34e-4bcf-b032-a0ec72b6f501",
   "metadata": {},
   "source": [
    "Extract year, month, day and hour from GMT string time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c681ca-bcc9-4993-aaec-3c8aa5b61456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forex['Year'] = df_forex['Gmt time'].apply(lambda x: int(x[6:10]))\n",
    "df_forex['Month'] = df_forex['Gmt time'].apply(lambda x: int(x[3:5]))\n",
    "df_forex['Day'] = df_forex['Gmt time'].apply(lambda x: int(x[0:2]))\n",
    "df_forex['Hour'] = df_forex['Gmt time'].apply(lambda x: int(x[11:13]))\n",
    "\n",
    "df_forex.drop(columns=['Gmt time'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f73319-7ef6-4233-adfc-1e72e0afab21",
   "metadata": {},
   "source": [
    "Create targets vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32b74bb-4794-4346-9b35-3465df36b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forex['Bid_Close_next'] = np.concatenate([df_forex.Bid_Close.to_numpy(), np.array([0])])[1:]\n",
    "df_forex['Ask_Close_next'] = np.concatenate([df_forex.Ask_Close.to_numpy(), np.array([0])])[1:]\n",
    "df_forex = df_forex.iloc[0:-1, :]\n",
    "\n",
    "y_bid = df_forex['Bid_Close_next'].to_numpy()\n",
    "y_ask = df_forex['Ask_Close_next'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562ebc3-7c70-4f4f-942c-3e01d2089681",
   "metadata": {},
   "source": [
    "Create features vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4653ba1b-17e4-4e7e-9ce0-021d8918580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_previous_data_from_timelaps(X):\n",
    "    \n",
    "    return np.array([[\n",
    "        np.max(X[:, 0]), np.min(X[:, 1]), X[0, 2], np.sum(X[:, 3]), np.max(X[:, 4]), np.min(X[:, 5]), X[0, 6], np.sum(X[:, 7])\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "844b1174-07da-471d-b116-0bf36979328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_date = df_forex[['Year', 'Month', 'Day', 'Hour']].to_numpy()\n",
    "X_now = df_forex[['Ask_High', 'Ask_Low', 'Ask_Close', 'Ask_Volume', 'Bid_High', 'Bid_Low', 'Bid_Close', 'Bid_Volume']].to_numpy()\n",
    "X_previous_hour = np.concatenate([X_now[N_PREVIOUS_HOUR_VALUES - 1 - i: X_now.shape[0] - 1 - i] for i in range(N_PREVIOUS_HOUR_VALUES)], axis=1)\n",
    "X_previous_day = np.concatenate([\n",
    "    np.concatenate([\n",
    "        concat_previous_data_from_timelaps(X_now[(N_PREVIOUS_DAY_VALUES - 1 - i) * 24 + line: (N_PREVIOUS_DAY_VALUES - i) * 24 + line]) \n",
    "        for i in range(N_PREVIOUS_DAY_VALUES)], \n",
    "        axis=1)\n",
    "    for line in range(X_now.shape[0] - N_PREVIOUS_DAY_VALUES * 24)],\n",
    "    axis=0)\n",
    "X_previous_week = np.concatenate([\n",
    "    np.concatenate([\n",
    "        concat_previous_data_from_timelaps(X_now[(N_PREVIOUS_WEEK_VALUES - 1 - i) * 24 * 7 + line: (N_PREVIOUS_WEEK_VALUES - i) * 24 * 7 + line]) \n",
    "        for i in range(N_PREVIOUS_WEEK_VALUES)], \n",
    "        axis=1)\n",
    "    for line in range(X_now.shape[0] - N_PREVIOUS_WEEK_VALUES * 24 * 7)],\n",
    "    axis=0)\n",
    "X_previous_month = np.concatenate([\n",
    "    np.concatenate([\n",
    "        concat_previous_data_from_timelaps(X_now[(N_PREVIOUS_MONTH_VALUES - 1 - i) * 24 * 30 + line: (N_PREVIOUS_MONTH_VALUES - i) * 24 * 30 + line]) \n",
    "        for i in range(N_PREVIOUS_MONTH_VALUES)], \n",
    "        axis=1)\n",
    "    for line in range(X_now.shape[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)],\n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f981bdc-031d-44f4-b2c1-a3c52bb50fcf",
   "metadata": {},
   "source": [
    "Remove offset to year, month and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b1199e3-8781-4be2-961e-f46c700fb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_date[:, 0] = X_date[:, 0] - 2013\n",
    "X_date[:, 1] = X_date[:, 1] - 1\n",
    "X_date[:, 2] = X_date[:, 2] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439f01c-951e-4b89-bb6d-386c4d7940fa",
   "metadata": {},
   "source": [
    "Remove N_PREVIOUS_MONTH_VALUES to features and targets vectors (since there is not historic before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c84c16-14ec-4ce7-91db-7ff52e0f775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_date = X_date[X_date.shape[0] - X_previous_month.shape[0]:]\n",
    "X_now = X_now[X_now.shape[0] - X_previous_month.shape[0]:]\n",
    "X_previous_hour = X_previous_hour[X_previous_hour.shape[0] - X_previous_month.shape[0]:]\n",
    "X_previous_day = X_previous_day[X_previous_day.shape[0] - X_previous_month.shape[0]:]\n",
    "X_previous_week = X_previous_week[X_previous_week.shape[0] - X_previous_week.shape[0]:]\n",
    "y_bid = y_bid[y_bid.shape[0] - X_previous_month.shape[0]:]\n",
    "y_ask = y_ask[y_ask.shape[0] - X_previous_month.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07d266-2400-48fd-84c2-d85acce83bf1",
   "metadata": {},
   "source": [
    "Create index for tuning and evaluation training/validation/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b8f222f-775d-4999-bf22-39e706cdffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tuning_trains = [\n",
    "    range(0, df_forex[(df_forex.Year == int(2020 + i / 2)) & (df_forex.Month == 1 + ((6 * i) % 12)) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)\n",
    "    for i in range(4)\n",
    "]\n",
    "\n",
    "idx_tuning_validations = [\n",
    "    range(\n",
    "        df_forex[(df_forex.Year == int(2020 + i / 2)) & (df_forex.Month == 1 + ((6 * i) % 12)) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30,\n",
    "        df_forex[(df_forex.Year == int(2020 + (i + 1) / 2)) & (df_forex.Month == 1 + ((6 * (i + 1)) % 12)) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)\n",
    "    for i in range(4)\n",
    "]\n",
    "\n",
    "idx_eval_train = range(0, df_forex[(df_forex.Year == 2021) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)\n",
    "\n",
    "idx_eval_validation = range(\n",
    "    df_forex[(df_forex.Year == 2021) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30,\n",
    "    df_forex[(df_forex.Year == 2022) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)\n",
    "\n",
    "idx_eval_test = range(\n",
    "    df_forex[(df_forex.Year == 2022) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30,\n",
    "    df_forex[(df_forex.Year == 2023) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d09b8-4f46-40a3-b520-8c9f18163ff3",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c95613-5dfd-4aec-9529-6978bb160198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx, \n",
    "                 n_previous_hour_values, n_previous_day_values, n_previous_week_values, n_previous_month_values, n_features):\n",
    "        \n",
    "        self.X_date = X_date[idx].astype(np.int32)\n",
    "        self.X_now = X_now[idx].astype(np.float32)\n",
    "        self.X_previous_hour = X_previous_hour[idx].astype(np.float32)\n",
    "        self.X_previous_day = X_previous_day[idx].astype(np.float32)\n",
    "        self.X_previous_week = X_previous_week[idx].astype(np.float32)\n",
    "        self.X_previous_month = X_previous_month[idx].astype(np.float32)\n",
    "        self.y_bid = y_bid[idx].astype(np.float32)\n",
    "        self.y_ask = y_ask[idx].astype(np.float32)\n",
    "        self.n_previous_hour_values = n_previous_hour_values\n",
    "        self.n_previous_day_values = n_previous_day_values\n",
    "        self.n_previous_week_values = n_previous_week_values\n",
    "        self.n_previous_month_values = n_previous_month_values\n",
    "        self.n_features = n_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.y_bid.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.X_date[idx], self.X_now[idx], self.X_previous_hour[idx], self.X_previous_day[idx], \\\n",
    "               self.X_previous_week[idx], self.X_previous_month[idx], self.y_bid[idx], self.y_ask[idx]\n",
    "    \n",
    "    def fit_scalers(self, scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask):\n",
    "        \n",
    "        return scaler_now.fit(self.X_now), scaler_previous_hour.fit(self.X_previous_hour), scaler_previous_day.fit(self.X_previous_day), \\\n",
    "               scaler_previous_week.fit(self.X_previous_week), scaler_previous_month.fit(self.X_previous_month), \\\n",
    "               scaler_y_bid.fit(np.expand_dims(self.y_bid, axis=1)), scaler_y_ask.fit(np.expand_dims(self.y_ask, axis=1))\n",
    "    \n",
    "    def scale(self, scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask):\n",
    "        \n",
    "        self.X_now = scaler_now.transform(self.X_now)\n",
    "        self.X_previous_hour = scaler_previous_hour.transform(self.X_previous_hour).reshape(self.X_previous_hour.shape[0], self.n_previous_hour_values, self.n_features)\n",
    "        self.X_previous_day = scaler_previous_day.transform(self.X_previous_day).reshape(self.X_previous_day.shape[0], self.n_previous_day_values, self.n_features)\n",
    "        self.X_previous_week = scaler_previous_week.transform(self.X_previous_week).reshape(self.X_previous_week.shape[0], self.n_previous_week_values, self.n_features)\n",
    "        self.X_previous_month = scaler_previous_month.transform(self.X_previous_month).reshape(self.X_previous_month.shape[0], self.n_previous_month_values, self.n_features)\n",
    "        self.y_bid = np.squeeze(scaler_y_bid.transform(np.expand_dims(self.y_bid, axis=1)))\n",
    "        self.y_ask = np.squeeze(scaler_y_ask.transform(np.expand_dims(self.y_ask, axis=1)))\n",
    "        \n",
    "    def transfer_to_tensor(self):\n",
    "        \n",
    "        self.X_date = torch.from_numpy(self.X_date)\n",
    "        self.X_now = torch.from_numpy(self.X_now)\n",
    "        self.X_previous_hour = torch.from_numpy(self.X_previous_hour)\n",
    "        self.X_previous_day = torch.from_numpy(self.X_previous_day)\n",
    "        self.X_previous_week = torch.from_numpy(self.X_previous_week)\n",
    "        self.X_previous_month = torch.from_numpy(self.X_previous_month)\n",
    "        self.y_bid = torch.from_numpy(self.y_bid)\n",
    "        self.y_ask = torch.from_numpy(self.y_ask)\n",
    "        \n",
    "    def cuda(self):\n",
    "        \n",
    "        self.X_date = self.X_date.cuda()\n",
    "        self.X_now = self.X_now.cuda()\n",
    "        self.X_previous_hour = self.X_previous_hour.cuda()\n",
    "        self.X_previous_day = self.X_previous_day.cuda()\n",
    "        self.X_previous_week = self.X_previous_week.cuda()\n",
    "        self.X_previous_month = self.X_previous_month.cuda()\n",
    "        self.y_bid = self.y_bid.cuda()\n",
    "        self.y_ask = self.y_ask.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        \n",
    "        self.X_date = self.X_date.cpu()\n",
    "        self.X_now = self.X_now.cpu()\n",
    "        self.X_previous_hour = self.X_previous_hour.cpu()\n",
    "        self.X_previous_day = self.X_previous_day.cpu()\n",
    "        self.X_previous_week = self.X_previous_week.cpu()\n",
    "        self.X_previous_month = self.X_previous_month.cpu()\n",
    "        self.y_bid = self.y_bid.cpu()\n",
    "        self.y_ask = self.y_ask.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fb184-b947-4f46-beb7-17d8f994eff8",
   "metadata": {},
   "source": [
    "Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e953f82-17d8-4f9f-a07c-82d2ee0dbcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tuning_trains = [ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES) for idx in idx_tuning_trains]\n",
    "dataset_tuning_validations = [ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES) for idx in idx_tuning_validations]\n",
    "dataset_eval_train = ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx_eval_train, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES)\n",
    "dataset_eval_validation = ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx_eval_validation, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES)\n",
    "dataset_eval_test = ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx_eval_test, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b04f3-1bfc-4f86-b3fc-1cc4d4d1e644",
   "metadata": {},
   "source": [
    "Scale datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e89224b0-a3b4-4f62-8a96-5f4a42e7175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_tuning_train, dataset_tuning_validation in zip(dataset_tuning_trains, dataset_tuning_validations):\n",
    "    \n",
    "    scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask = dataset_tuning_train.fit_scalers(MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler())\n",
    "    dataset_tuning_train.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)\n",
    "    dataset_tuning_validation.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)\n",
    "    \n",
    "scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask = dataset_eval_train.fit_scalers(MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler())\n",
    "dataset_eval_train.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)\n",
    "dataset_eval_validation.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)\n",
    "dataset_eval_test.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de283f-4491-4aff-b37d-58f13ba07010",
   "metadata": {},
   "source": [
    "Transfer to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3597679-aea6-4825-be9b-8d91e7e490a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_tuning_train, dataset_tuning_validation in zip(dataset_tuning_trains, dataset_tuning_validations):\n",
    "    \n",
    "    dataset_tuning_train.transfer_to_tensor()\n",
    "    dataset_tuning_validation.transfer_to_tensor()\n",
    "    \n",
    "dataset_eval_train.transfer_to_tensor()\n",
    "dataset_eval_validation.transfer_to_tensor()\n",
    "dataset_eval_test.transfer_to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e9dd2-5b8b-48e4-9dfe-9bf3378ada18",
   "metadata": {},
   "source": [
    "Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "436b3904-992c-42d3-a50e-fc850b588087",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data preprocessed/dataset_tuning_train_0.pt', 'wb') as file:\n",
    "    torch.save(dataset_tuning_trains[0], file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_tuning_train_1.pt', 'wb') as file:\n",
    "    torch.save(dataset_tuning_trains[1], file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_tuning_train_2.pt', 'wb') as file:\n",
    "    torch.save(dataset_tuning_trains[2], file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_tuning_train_3.pt', 'wb') as file:\n",
    "    torch.save(dataset_tuning_trains[3], file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_tuning_validation_0.pt', 'wb') as file:\n",
    "    torch.save(dataset_tuning_validations[0], file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_tuning_validation_1.pt', 'wb') as file:\n",
    "    torch.save(dataset_tuning_validations[1], file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_tuning_validation_2.pt', 'wb') as file:\n",
    "    torch.save(dataset_tuning_validations[2], file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_tuning_validation_3.pt', 'wb') as file:\n",
    "    torch.save(dataset_tuning_validations[3], file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_eval_train.pt', 'wb') as file:\n",
    "    torch.save(dataset_eval_train, file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_eval_validation.pt', 'wb') as file:\n",
    "    torch.save(dataset_eval_validation, file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open('data preprocessed/dataset_eval_test.pt', 'wb') as file:\n",
    "    torch.save(dataset_eval_test, file, pickle_module=pickle, pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5944eb4-9e4f-4355-9ce9-8970dbe0ebb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Y bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e4640bf-0f47-434f-9091-9f41b0ebafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'y_bid'\n",
    "mlp.set_experiment(\"Forex EUR CHF Bid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b79c47-2019-4590-b06b-3945bbbe4d2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP 0 layer hour memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a62198aa-ce82-471b-84f5-448e44f61150",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'MLP0HourMemory'\n",
    "MODEL_VERSION = 1.0\n",
    "TUNING_PATIENCE = 5\n",
    "TUNING_EPOCHS = 50\n",
    "TESTING_PATIENCE = 10\n",
    "TESTING_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd37b7-9881-4cd1-98db-34ec2ef3241e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ac642-f3e7-4aa9-bb1b-25133e151e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(REPOSITORY_STUDIES, TARGET, 'study ' + TARGET + ' ' + MODEL_NAME + '.pkl')):\n",
    "    with open(os.path.join(REPOSITORY_STUDIES, TARGET, 'study ' + TARGET + ' ' + MODEL_NAME + '.pkl'), 'rb') as file:\n",
    "        study = pickle.load(file)\n",
    "else:\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    \n",
    "while True:\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial, \n",
    "            dataset_tuning_trains, \n",
    "            dataset_tuning_validations, \n",
    "            MODEL_NAME, \n",
    "            TARGET, \n",
    "            patience=TUNING_PATIENCE, \n",
    "            epochs=TUNING_EPOCHS),\n",
    "        n_trials=1, \n",
    "        timeout=None, \n",
    "        n_jobs=1)\n",
    "    \n",
    "    with open(os.path.join(REPOSITORY_STUDIES, TARGET, 'study ' + TARGET + ' ' + MODEL_NAME + '.pkl'), 'wb') as file:\n",
    "        pickle.dump(study, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a437aa8-936a-40d6-a8a6-b8033f46fcb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559bd5eb-2443-4722-8482-37728cb08b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(REPOSITORY_STUDIES, TARGET, 'study ' + TARGET + ' ' + MODEL_NAME + '.pkl')):\n",
    "    with open(os.path.join(REPOSITORY_STUDIES, TARGET, 'study ' + TARGET + ' ' + MODEL_NAME + '.pkl'), 'rb') as file:\n",
    "        study = pickle.load(file)\n",
    "else:\n",
    "    raise Exception(\"Study do not exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3570e29-759a-4722-b2b8-e7099c78f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058dbd3-3c97-4a1e-8811-6f9f33b3f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc514ff2-07dd-4d95-b5eb-96150fa8c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca5257-30f0-4776-88ea-1db9e98acb51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62b2221d-9946-4b1a-bb96-e22acd326858",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(REPOSITORY_STUDIES, TARGET, 'study ' + TARGET + ' ' + MODEL_NAME + '.pkl')):\n",
    "    with open(os.path.join(REPOSITORY_STUDIES, TARGET, 'study ' + TARGET + ' ' + MODEL_NAME + '.pkl'), 'rb') as file:\n",
    "        study = pickle.load(file)\n",
    "else:\n",
    "    raise Exception(\"Study do not exists\")\n",
    "\n",
    "model = get_model(MODEL_NAME, **study.best_params)\n",
    "\n",
    "model, loss_test_mse, loss_test_mae, loss_test_mse_unscaled, loss_test_mae_unscaled, relative_error, max_error_absolute, max_error_relative, epoch = evaluate_for_testing(\n",
    "    model, \n",
    "    dataset_eval_train, \n",
    "    dataset_eval_validation, \n",
    "    dataset_eval_test, \n",
    "    scaler_target=scaler_y_bid if TARGET == 'y_bid' else scaler_y_ask, \n",
    "    target=TARGET, \n",
    "    optimizer=study.best_params['optimizer'], \n",
    "    batch_size_train=study.best_params['batch_size_train'], \n",
    "    batch_size_validation=dataset_eval_validation.__len__(), \n",
    "    batch_size_test=dataset_eval_test.__len__(), \n",
    "    learning_rate=study.best_params['learning_rate'], \n",
    "    weight_decay=study.best_params['weight_decay'], \n",
    "    patience=TESTING_PATIENCE, \n",
    "    epochs=TESTING_EPOCHS)\n",
    "\n",
    "mlp.set_version(MODEL_VERSION)\n",
    "\n",
    "with mlp.run():\n",
    "    \n",
    "    mlp.pytorch.register_model(MODEL_NAME, model)\n",
    "    mlp.log_params({\n",
    "        \"n_previous_hour_values\": study.best_params.get(\"n_previous_hour_values\", 0),\n",
    "        \"n_previous_day_values\": study.best_params.get(\"n_previous_day_values\", 0),\n",
    "        \"n_previous_week_values\": study.best_params.get(\"n_previous_week_values\", 0),\n",
    "        \"n_previous_month_values\": study.best_params.get(\"n_previous_month_values\", 0),\n",
    "        \"optimizer\": study.best_params['optimizer'],\n",
    "        \"batch_size_train\": int(2 ** study.best_params['batch_size_train']),\n",
    "        \"learning_rate\": study.best_params['learning_rate'],\n",
    "        \"weight_decay\": study.best_params['weight_decay'],\n",
    "        \"patience\": TESTING_PATIENCE,\n",
    "        \"epochs\": TESTING_EPOCHS,\n",
    "        \"effective epochs\": epoch\n",
    "    })\n",
    "    mlp.log_metrics({\n",
    "        \"MAE normalized * 1e6\": loss_test_mse * 1e6,\n",
    "        \"MSE normalized * 1e6\": loss_test_mae * 1e6,\n",
    "        \"MAE absolute * 1e6\": loss_test_mse_unscaled * 1e6,\n",
    "        \"MSE absolute * 1e6\": loss_test_mae_unscaled * 1e6,\n",
    "        \"Relative error * 1e6\": relative_error * 1e6,\n",
    "        \"Max error absolute * 1e6\": max_error_absolute * 1e6,\n",
    "        \"Max error relative * 1e6\": max_error_relative * 1e6\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825a585-4a27-4d90-97bf-89207f639e65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Y ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9fd5d-24b5-458d-91bb-024825f2dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'y_ask'\n",
    "mlp.set_experiment(\"Forex EUR CHF Ask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abfc47-5902-484d-965c-686d62f8f31c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UI MlOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479dc04e-797a-4efb-b07b-b3f5e1c4632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ui(host='0.0.0.0', port=8085)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c8325-1401-4ea2-9fd8-3064a0c88e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
