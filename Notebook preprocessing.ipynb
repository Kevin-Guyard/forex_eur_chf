{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2c9249-8ead-44fd-9c28-d898a955fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b49ea16-ecd3-41fd-af78-814ff50ebdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_DATA_RAW = 'data raw'\n",
    "REPOSITORY_DATA_PREPROCESSED = 'data preprocessed'\n",
    "REPOSITORY_STUDIES = 'studies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d849c83-618b-4d89-837c-0ad644cb95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PREVIOUS_HOUR_VALUES = 720\n",
    "N_PREVIOUS_DAY_VALUES = 90\n",
    "N_PREVIOUS_WEEK_VALUES = 50\n",
    "N_PREVIOUS_MONTH_VALUES = 36\n",
    "N_FEATURES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848c313-613b-4494-bf59-2c3f6622e982",
   "metadata": {},
   "source": [
    "Read raw dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e19c694-e7f4-4887-ac8f-54ae42bea5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ask = pd.read_csv(os.path.join(REPOSITORY_DATA_RAW, 'FOREX_EUR_CHF_ASK.csv'))\n",
    "df_bid = pd.read_csv(os.path.join(REPOSITORY_DATA_RAW, 'FOREX_EUR_CHF_BID.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d459e-57dc-44e4-9a0b-0eb0ad30f0ca",
   "metadata": {},
   "source": [
    "Rename columns with bid and ask prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c82d44-b787-4cf0-8eb9-a82c6375f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ask.rename(\n",
    "    columns={\n",
    "        'Open': 'Ask_Open',\n",
    "        'High': 'Ask_High',\n",
    "        'Low': 'Ask_Low',\n",
    "        'Close': 'Ask_Close',\n",
    "        'Volume': 'Ask_Volume'\n",
    "    },\n",
    "    inplace=True)\n",
    "\n",
    "df_bid.rename(\n",
    "    columns={\n",
    "        'Open': 'Bid_Open',\n",
    "        'High': 'Bid_High',\n",
    "        'Low': 'Bid_Low',\n",
    "        'Close': 'Bid_Close',\n",
    "        'Volume': 'Bid_Volume'\n",
    "    },\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f24fba-3506-40c0-972f-a3e6e2245230",
   "metadata": {},
   "source": [
    "Merge dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c27f13e3-1239-4a52-9fe6-615903685107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forex = df_ask.merge(df_bid, left_on='Gmt time', right_on='Gmt time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91b088-f972-45f0-8eb4-50f90abd302d",
   "metadata": {},
   "source": [
    "Extract year, month, day and hour from GMT string time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de818e7-51bd-4e3c-a36d-aa2079c0a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forex['Year'] = df_forex['Gmt time'].apply(lambda x: int(x[6:10]))\n",
    "df_forex['Month'] = df_forex['Gmt time'].apply(lambda x: int(x[3:5]))\n",
    "df_forex['Day'] = df_forex['Gmt time'].apply(lambda x: int(x[0:2]))\n",
    "df_forex['Hour'] = df_forex['Gmt time'].apply(lambda x: int(x[11:13]))\n",
    "\n",
    "df_forex.drop(columns=['Gmt time'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4ab23-aa83-4e12-b238-5f9a253af52a",
   "metadata": {},
   "source": [
    "Create targets vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72ae691-baa0-4326-9084-84a0442b2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forex['Bid_Close_next'] = np.concatenate([df_forex.Bid_Close.to_numpy(), np.array([0])])[1:]\n",
    "df_forex['Ask_Close_next'] = np.concatenate([df_forex.Ask_Close.to_numpy(), np.array([0])])[1:]\n",
    "df_forex = df_forex.iloc[0:-1, :]\n",
    "\n",
    "y_bid = df_forex['Bid_Close_next'].to_numpy()\n",
    "y_ask = df_forex['Ask_Close_next'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde898fe-3610-42aa-aa1c-cef58cf168eb",
   "metadata": {},
   "source": [
    "Create features vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "190fd1a5-7ee8-4589-b85e-4956bdda8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_previous_data_from_timelaps(X):\n",
    "    \n",
    "    return np.array([[\n",
    "        np.max(X[:, 0]), np.min(X[:, 1]), X[0, 2], np.sum(X[:, 3]), np.max(X[:, 4]), np.min(X[:, 5]), X[0, 6], np.sum(X[:, 7])\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd236f6-5de3-4e24-876d-346d0d1a539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_date = df_forex[['Year', 'Month', 'Day', 'Hour']].to_numpy()\n",
    "X_now = df_forex[['Ask_High', 'Ask_Low', 'Ask_Close', 'Ask_Volume', 'Bid_High', 'Bid_Low', 'Bid_Close', 'Bid_Volume']].to_numpy()\n",
    "X_previous_hour = np.concatenate([X_now[N_PREVIOUS_HOUR_VALUES - 1 - i: X_now.shape[0] - 1 - i] for i in range(N_PREVIOUS_HOUR_VALUES)], axis=1)\n",
    "X_previous_day = np.concatenate([\n",
    "    np.concatenate([\n",
    "        concat_previous_data_from_timelaps(X_now[(N_PREVIOUS_DAY_VALUES - 1 - i) * 24 + line: (N_PREVIOUS_DAY_VALUES - i) * 24 + line]) \n",
    "        for i in range(N_PREVIOUS_DAY_VALUES)], \n",
    "        axis=1)\n",
    "    for line in range(X_now.shape[0] - N_PREVIOUS_DAY_VALUES * 24)],\n",
    "    axis=0)\n",
    "X_previous_week = np.concatenate([\n",
    "    np.concatenate([\n",
    "        concat_previous_data_from_timelaps(X_now[(N_PREVIOUS_WEEK_VALUES - 1 - i) * 24 * 7 + line: (N_PREVIOUS_WEEK_VALUES - i) * 24 * 7 + line]) \n",
    "        for i in range(N_PREVIOUS_WEEK_VALUES)], \n",
    "        axis=1)\n",
    "    for line in range(X_now.shape[0] - N_PREVIOUS_WEEK_VALUES * 24 * 7)],\n",
    "    axis=0)\n",
    "X_previous_month = np.concatenate([\n",
    "    np.concatenate([\n",
    "        concat_previous_data_from_timelaps(X_now[(N_PREVIOUS_MONTH_VALUES - 1 - i) * 24 * 30 + line: (N_PREVIOUS_MONTH_VALUES - i) * 24 * 30 + line]) \n",
    "        for i in range(N_PREVIOUS_MONTH_VALUES)], \n",
    "        axis=1)\n",
    "    for line in range(X_now.shape[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)],\n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2210e53-f833-4183-b749-eb5e02b4339b",
   "metadata": {},
   "source": [
    "Remove offset to year, month and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e83c0c-0961-4eca-b33f-bb7d7098c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_date[:, 0] = X_date[:, 0] - 2013\n",
    "X_date[:, 1] = X_date[:, 1] - 1\n",
    "X_date[:, 2] = X_date[:, 2] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3315495-e5b8-482a-a8a2-cc6b204a48bc",
   "metadata": {},
   "source": [
    "Remove N_PREVIOUS_MONTH_VALUES to features and targets vectors (since there is not historic before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34dc570c-f47d-423e-9197-eace12dc77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_date = X_date[X_date.shape[0] - X_previous_month.shape[0]:]\n",
    "X_now = X_now[X_now.shape[0] - X_previous_month.shape[0]:]\n",
    "X_previous_hour = X_previous_hour[X_previous_hour.shape[0] - X_previous_month.shape[0]:]\n",
    "X_previous_day = X_previous_day[X_previous_day.shape[0] - X_previous_month.shape[0]:]\n",
    "X_previous_week = X_previous_week[X_previous_week.shape[0] - X_previous_week.shape[0]:]\n",
    "y_bid = y_bid[y_bid.shape[0] - X_previous_month.shape[0]:]\n",
    "y_ask = y_ask[y_ask.shape[0] - X_previous_month.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724ed34-87c3-4abe-ae49-eb24ee5015c8",
   "metadata": {},
   "source": [
    "Create index for tuning and evaluation training/validation/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e66ba85-4acf-4b9d-be81-d0ffad76e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tuning_trains = [\n",
    "    range(0, df_forex[(df_forex.Year == int(2020 + i / 2)) & (df_forex.Month == 1 + ((6 * i) % 12)) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)\n",
    "    for i in range(4)\n",
    "]\n",
    "\n",
    "idx_tuning_validations = [\n",
    "    range(\n",
    "        df_forex[(df_forex.Year == int(2020 + i / 2)) & (df_forex.Month == 1 + ((6 * i) % 12)) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30,\n",
    "        df_forex[(df_forex.Year == int(2020 + (i + 1) / 2)) & (df_forex.Month == 1 + ((6 * (i + 1)) % 12)) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)\n",
    "    for i in range(4)\n",
    "]\n",
    "\n",
    "idx_eval_train = range(0, df_forex[(df_forex.Year == 2021) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)\n",
    "\n",
    "idx_eval_validation = range(\n",
    "    df_forex[(df_forex.Year == 2021) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30,\n",
    "    df_forex[(df_forex.Year == 2022) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)\n",
    "\n",
    "idx_eval_test = range(\n",
    "    df_forex[(df_forex.Year == 2022) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30,\n",
    "    df_forex[(df_forex.Year == 2023) & (df_forex.Month == 1) & (df_forex.Day == 1) & (df_forex.Hour == 0)].index[0] - N_PREVIOUS_MONTH_VALUES * 24 * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323e87e-b3db-45ec-913d-6c3a844925af",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de1b3f9f-f7a2-41ca-bc2d-8b2cceeaa520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx, \n",
    "                 n_previous_hour_values, n_previous_day_values, n_previous_week_values, n_previous_month_values, n_features):\n",
    "        \n",
    "        self.X_date = X_date[idx].astype(np.int32)\n",
    "        self.X_now = X_now[idx].astype(np.float32)\n",
    "        self.X_previous_hour = X_previous_hour[idx].astype(np.float32)\n",
    "        self.X_previous_day = X_previous_day[idx].astype(np.float32)\n",
    "        self.X_previous_week = X_previous_week[idx].astype(np.float32)\n",
    "        self.X_previous_month = X_previous_month[idx].astype(np.float32)\n",
    "        self.y_bid = y_bid[idx].astype(np.float32)\n",
    "        self.y_ask = y_ask[idx].astype(np.float32)\n",
    "        self.n_previous_hour_values = n_previous_hour_values\n",
    "        self.n_previous_day_values = n_previous_day_values\n",
    "        self.n_previous_week_values = n_previous_week_values\n",
    "        self.n_previous_month_values = n_previous_month_values\n",
    "        self.n_features = n_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.y_bid.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.X_date[idx], self.X_now[idx], self.X_previous_hour[idx], self.X_previous_day[idx], \\\n",
    "               self.X_previous_week[idx], self.X_previous_month[idx], self.y_bid[idx], self.y_ask[idx]\n",
    "    \n",
    "    def fit_scalers(self, scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask):\n",
    "        \n",
    "        return scaler_now.fit(self.X_now), scaler_previous_hour.fit(self.X_previous_hour), scaler_previous_day.fit(self.X_previous_day), \\\n",
    "               scaler_previous_week.fit(self.X_previous_week), scaler_previous_month.fit(self.X_previous_month), \\\n",
    "               scaler_y_bid.fit(np.expand_dims(self.y_bid, axis=1)), scaler_y_ask.fit(np.expand_dims(self.y_ask, axis=1))\n",
    "    \n",
    "    def scale(self, scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask):\n",
    "        \n",
    "        self.X_now = scaler_now.transform(self.X_now)\n",
    "        self.X_previous_hour = scaler_previous_hour.transform(self.X_previous_hour).reshape(self.X_previous_hour.shape[0], self.n_previous_hour_values, self.n_features)\n",
    "        self.X_previous_day = scaler_previous_day.transform(self.X_previous_day).reshape(self.X_previous_day.shape[0], self.n_previous_day_values, self.n_features)\n",
    "        self.X_previous_week = scaler_previous_week.transform(self.X_previous_week).reshape(self.X_previous_week.shape[0], self.n_previous_week_values, self.n_features)\n",
    "        self.X_previous_month = scaler_previous_month.transform(self.X_previous_month).reshape(self.X_previous_month.shape[0], self.n_previous_month_values, self.n_features)\n",
    "        self.y_bid = np.squeeze(scaler_y_bid.transform(np.expand_dims(self.y_bid, axis=1)))\n",
    "        self.y_ask = np.squeeze(scaler_y_ask.transform(np.expand_dims(self.y_ask, axis=1)))\n",
    "        \n",
    "    def transfer_to_tensor(self):\n",
    "        \n",
    "        self.X_date = torch.from_numpy(self.X_date)\n",
    "        self.X_now = torch.from_numpy(self.X_now)\n",
    "        self.X_previous_hour = torch.from_numpy(self.X_previous_hour)\n",
    "        self.X_previous_day = torch.from_numpy(self.X_previous_day)\n",
    "        self.X_previous_week = torch.from_numpy(self.X_previous_week)\n",
    "        self.X_previous_month = torch.from_numpy(self.X_previous_month)\n",
    "        self.y_bid = torch.from_numpy(self.y_bid)\n",
    "        self.y_ask = torch.from_numpy(self.y_ask)\n",
    "        \n",
    "    def cuda(self):\n",
    "        \n",
    "        self.X_date = self.X_date.cuda()\n",
    "        self.X_now = self.X_now.cuda()\n",
    "        self.X_previous_hour = self.X_previous_hour.cuda()\n",
    "        self.X_previous_day = self.X_previous_day.cuda()\n",
    "        self.X_previous_week = self.X_previous_week.cuda()\n",
    "        self.X_previous_month = self.X_previous_month.cuda()\n",
    "        self.y_bid = self.y_bid.cuda()\n",
    "        self.y_ask = self.y_ask.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        \n",
    "        self.X_date = self.X_date.cpu()\n",
    "        self.X_now = self.X_now.cpu()\n",
    "        self.X_previous_hour = self.X_previous_hour.cpu()\n",
    "        self.X_previous_day = self.X_previous_day.cpu()\n",
    "        self.X_previous_week = self.X_previous_week.cpu()\n",
    "        self.X_previous_month = self.X_previous_month.cpu()\n",
    "        self.y_bid = self.y_bid.cpu()\n",
    "        self.y_ask = self.y_ask.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f830cb5-54bd-45e0-b742-6f6a44070d32",
   "metadata": {},
   "source": [
    "Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3185429a-0db0-4bfe-bdf7-457096432f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tuning_trains = [ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES) for idx in idx_tuning_trains]\n",
    "dataset_tuning_validations = [ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES) for idx in idx_tuning_validations]\n",
    "dataset_eval_train = ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx_eval_train, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES)\n",
    "dataset_eval_validation = ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx_eval_validation, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES)\n",
    "dataset_eval_test = ForexDataset(X_date, X_now, X_previous_hour, X_previous_day, X_previous_week, X_previous_month, y_bid, y_ask, idx_eval_test, N_PREVIOUS_HOUR_VALUES, N_PREVIOUS_DAY_VALUES, N_PREVIOUS_WEEK_VALUES, N_PREVIOUS_MONTH_VALUES, N_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e305e-c5fa-4c75-b3ce-dfc5ee46c4c8",
   "metadata": {},
   "source": [
    "Scale datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d2f8e7-0c2a-4947-a17d-5ec021c5535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_tuning_train, dataset_tuning_validation in zip(dataset_tuning_trains, dataset_tuning_validations):\n",
    "    \n",
    "    scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask = dataset_tuning_train.fit_scalers(MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler())\n",
    "    dataset_tuning_train.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)\n",
    "    dataset_tuning_validation.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)\n",
    "    \n",
    "scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask = dataset_eval_train.fit_scalers(MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler(), MinMaxScaler())\n",
    "dataset_eval_train.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)\n",
    "dataset_eval_validation.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)\n",
    "dataset_eval_test.scale(scaler_now, scaler_previous_hour, scaler_previous_day, scaler_previous_week, scaler_previous_month, scaler_y_bid, scaler_y_ask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719044a1-51c6-47ee-a5c3-47376ef29f0a",
   "metadata": {},
   "source": [
    "Transfer to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b305f2b6-dbd7-44e4-b85a-a898d5248efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_tuning_train, dataset_tuning_validation in zip(dataset_tuning_trains, dataset_tuning_validations):\n",
    "    \n",
    "    dataset_tuning_train.transfer_to_tensor()\n",
    "    dataset_tuning_validation.transfer_to_tensor()\n",
    "    \n",
    "dataset_eval_train.transfer_to_tensor()\n",
    "dataset_eval_validation.transfer_to_tensor()\n",
    "dataset_eval_test.transfer_to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ade2b-d282-46fe-8660-4f6572aea7d0",
   "metadata": {},
   "source": [
    "Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8dab59-a42b-43bc-9413-e0c538d49327",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    \n",
    "    with open(os.path.join(REPOSITORY_DATA_PREPROCESSED, 'dataset_tuning_train_' + str(i) + '.pt'), 'wb') as file:\n",
    "        torch.save(dataset_tuning_trains[i], file, pickle_module=pickle, pickle_protocol=4)\n",
    "        \n",
    "    with open(os.path.join(REPOSITORY_DATA_PREPROCESSED, 'dataset_tuning_validation_' + str(i) + '.pt'), 'wb') as file:\n",
    "        torch.save(dataset_tuning_validations[i], file, pickle_module=pickle, pickle_protocol=4)\n",
    "\n",
    "with open(os.path.join(REPOSITORY_DATA_PREPROCESSED, 'dataset_eval_train.pt'), 'wb') as file:\n",
    "    torch.save(dataset_eval_train, file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open(os.path.join(REPOSITORY_DATA_PREPROCESSED, 'dataset_eval_validation.pt'), 'wb') as file:\n",
    "    torch.save(dataset_eval_validation, file, pickle_module=pickle, pickle_protocol=4)\n",
    "with open(os.path.join(REPOSITORY_DATA_PREPROCESSED, 'dataset_eval_test.pt'), 'wb') as file:\n",
    "    torch.save(dataset_eval_test, file, pickle_module=pickle, pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006782f-4d8f-42c4-842a-eff4d1fa0e02",
   "metadata": {},
   "source": [
    "Save scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9284ca5-59d5-4d18-b7c2-99418df496b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(REPOSITORY_DATA_PREPROCESSED, 'scaler_y_bid.pkl'), 'wb') as file:\n",
    "    pickle.dump(scaler_y_bid, file)\n",
    "with open(os.path.join(REPOSITORY_DATA_PREPROCESSED, 'scaler_y_ask.pkl'), 'wb') as file:\n",
    "    pickle.dump(scaler_y_ask, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
